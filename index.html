<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no, viewport-fit=cover">
    <title>Gyro Shader Experience - Switchable</title>
    <style>
        body { margin: 0; font-family: sans-serif; overflow: hidden; background-color: #000; color: #fff; }
        #container { width: 100vw; height: 100vh; position: absolute; top: 0; left: 0; z-index: 1; }
        #videoBG {
            position: absolute; top: 0; left: 0; width: 100%; height: 100%;
            object-fit: cover; z-index: 0;
        }
        #controls { 
            display: block; /* Re-enable controls */
            position: absolute; top: 10px; left: 10px; z-index: 100;
            background: rgba(0,0,0,0.6); padding: 8px; border-radius: 8px;
        }
        #controls button {
            display: block; margin: 6px; padding: 10px 12px; color: white;
            background-color: #444; border: 1px solid #666; border-radius: 5px;
            cursor: pointer; font-size: 14px;
        }
        #controls button:hover { background-color: #666; }
        #controls button.active { background-color: #007bff; border-color: #0056b3;}

        #loadingMessage, #startExperienceUI { /* ... same ... */ }
    </style>

    <script type="importmap">
    {
      "imports": {
        "three": "https://cdnjs.cloudflare.com/ajax/libs/three.js/0.151.0/three.module.min.js",
        "three/addons/": "https://unpkg.com/three@0.151.0/examples/jsm/"
      }
    }
    </script>
</head>
<body>
    <div id="startExperienceUI"> </div>
    <div id="loadingMessage">Loading...</div>
    <video id="videoBG" autoplay playsinline muted style="display:none;"></video>
    <div id="controls" style="display:none;">
        <button id="btn-shader1" onclick="setActiveVisual('shader1')">Abstract Swirls</button>
        <button id="btn-waveform" onclick="setActiveVisual('waveform')">Waveform</button>
        <button id="btn-none" onclick="setActiveVisual('none')">Clear Visuals (Black)</button>
    </div>
    <div id="container"></div>

    <script type="module">
        import * as THREE from 'three';
        import { DeviceOrientationControls } from 'three/addons/controls/DeviceOrientationControls.js';

        let scene, camera, renderer, clock, deviceControls;
        let videoTexture, videoElement;
        
        let shaderPlane; // Single plane for all shader visuals
        let currentShaderMaterial; // The active shader material

        let currentVisualType = 'none'; // To track which shader is active

        const loadingMessage = document.getElementById('loadingMessage');
        const startExperienceUI = document.getElementById('startExperienceUI');
        const startArButton = document.getElementById('startArButton');
        const controlsDiv = document.getElementById('controls');
        videoElement = document.getElementById('videoBG');

        // --- Audio Analysis (same as before) ---
        let analyser; let dataArray;
        const audioSummary = { overallAverage: 0, bassAverage: 0, midAverage: 0, trebleAverage: 0, dataArrayValid: false, audioContextActive: false };
        async function startAudio() { /* ... same ... */ 
             if (audioSummary.audioContextActive) return;
            console.log("Requesting microphone access...");
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const source = audioContext.createMediaStreamSource(stream);
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256; 
                analyser.smoothingTimeConstant = 0.7;
                source.connect(analyser);
                const bufferLength = analyser.frequencyBinCount;
                dataArray = new Uint8Array(bufferLength);
                audioSummary.dataArrayValid = true;
                audioSummary.audioContextActive = true;
                console.log("Microphone access granted and analyser active.");
            } catch (err) {
                console.error("Error accessing microphone:", err);
                if(loadingMessage) loadingMessage.innerHTML += "<br><small>Microphone access denied. Audio reactivity disabled.</small>";
            }
        }
        function updateAudioSummary() { /* ... same, ensures 0-1 normalization ... */
            if (!analyser || !dataArray || !audioSummary.dataArrayValid) {
                audioSummary.overallAverage = 0; audioSummary.bassAverage = 0; audioSummary.midAverage = 0; audioSummary.trebleAverage = 0;
                return audioSummary;
            }
            analyser.getByteFrequencyData(dataArray);
            let S=0,B=0,M=0,T=0,l=dataArray.length,bc=Math.floor(l*0.15),mc=Math.floor(l*0.5);
            for(let i=0;i<l;i++){const v=dataArray[i]; S+=v; if(i<bc)B+=v;else if(i<mc)M+=v;else T+=v;}
            audioSummary.overallAverage=(S/l||0)/255; audioSummary.bassAverage=(bc>0?B/bc:0)/255; 
            audioSummary.midAverage=((mc-bc)>0?M/(mc-bc):0)/255; audioSummary.trebleAverage=((l-mc)>0?T/(l-mc):0)/255;
            return audioSummary;
        }
        
        async function initVideoStream() { /* ... same ... */ 
            console.log("initVideoStream: Attempting to get camera feed.");
            if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
                try {
                    const constraints = { video: { facingMode: 'environment' } };
                    const stream = await navigator.mediaDevices.getUserMedia(constraints);
                    videoElement.srcObject = stream;
                    await videoElement.play();
                    videoElement.style.display = 'block';
                    console.log("initVideoStream: Camera stream acquired and playing.");
                    
                    videoTexture = new THREE.VideoTexture(videoElement);
                    videoTexture.minFilter = THREE.LinearFilter;
                    videoTexture.magFilter = THREE.LinearFilter;
                    if (scene) scene.background = videoTexture;

                } catch (err) {
                    console.error("Error accessing camera:", err);
                    if (scene) scene.background = new THREE.Color(0x100015); 
                }
            } else {
                console.error("getUserMedia not supported!");
                if (scene) scene.background = new THREE.Color(0x100015);
            }
        }

        // --- Shader Definitions ---
        const commonVertexShader = `
            varying vec2 vUv;
            void main() {
                vUv = uv;
                // This makes the quad always fill the screen, regardless of camera position/rotation for the shader plane itself.
                // Useful if the shader plane is a child of the camera or if an orthographic camera is used for it.
                // For a plane in world space viewed by perspective camera, use standard projection:
                gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
            }
        `;

        const shaderVisuals = {
            'shader1': {
                fragmentShader: `
                    precision highp float;
                    uniform vec2 u_resolution;
                    uniform float u_time;
                    uniform float u_bass; uniform float u_mid; uniform float u_treble; uniform float u_overall_avg;
                    varying vec2 vUv;

                    void main() {
                        vec2 u = .2*( (vUv * 2.0 - 1.0) * u_resolution )/u_resolution.y;
                        vec4 z_shadertoy, o_shadertoy;
                        o_shadertoy = vec4(1.0,2.0,3.0,0.0); 
                        z_shadertoy = o_shadertoy;          
                        vec2 v_shadertoy = u_resolution.xy;
                        float t = u_time + u_bass * 5.0 + u_overall_avg * 2.0; // Audio influencing time

                        for (float a = .5, i = 0.0; 
                            ++i < 19.; 
                            o_shadertoy += (1. + cos(z_shadertoy+t)) 
                                / length((1.+i*dot(v_shadertoy,v_shadertoy)) 
                                        * sin(1.5*u/(.5-dot(u,u)) - 9.*u.yx + t))
                            )  
                            v_shadertoy = cos(++t - 7.*u*pow(a += .03 + u_mid*0.01, i)) - 5.*u, 
                            u += tanh(40. * dot(u *= mat2(cos(i + .02*t - vec4(0,11,33,0))) 
                                                    ,u) 
                                        * cos(1e2*u.yx + t + u_treble*3.0)) / 2e2
                            + .2 * a * u
                            + cos(4./exp(dot(o_shadertoy,o_shadertoy)/1e2) + t) / 3e2;
                            
                        o_shadertoy = 25.6 / (min(o_shadertoy, 13.) + 164. / o_shadertoy) 
                            - dot(u, u) / 250.;
                        gl_FragColor = vec4(o_shadertoy.rgb, 1.0);
                    }
                `,
                initUniforms: () => ({
                    u_time: { value: 0.0 },
                    u_resolution: { value: new THREE.Vector2(window.innerWidth, window.innerHeight) },
                    u_bass: { value: 0.0 }, u_mid: { value: 0.0 }, u_treble: { value: 0.0 }, u_overall_avg: { value: 0.0 }
                })
            },
            'waveform': {
                fragmentShader: `
                    precision highp float;
                    uniform vec2 u_resolution;
                    uniform float u_time;
                    uniform float u_bass; uniform float u_mid; uniform float u_treble; uniform float u_overall_avg;
                    varying vec2 vUv;

                    // "Waveform" by @XorDev - Adapted
                    void main() {
                        vec2 R = u_resolution.xy;
                        vec2 U = vUv * R; // Current fragment coord in pixels

                        vec4 O_accum = vec4(0.0); // Accumulator for color
                        float i_loop = 0.0, d_step, z_depth = 0.0, r_reflect;
                        
                        // Modified audio influence:
                        float time_adjusted = u_time + u_overall_avg * 3.0;

                        for(; i_loop++<90.0; // Raymarch 90 steps
                            // Accumulate color (Pick color and attenuate)
                            O_accum += (cos(z_depth*.5 + time_adjusted + u_bass * 2.0 + vec4(0,2,4,3)) + 1.3 ) / d_step / z_depth
                        ) {
                            // Raymarch sample point 'p'
                            // Original: vec3 p = z * normalize(vec3(I+I,0) - iResolution.xyy);
                            // I = fragCoord = vUv * u_resolution
                            // iResolution.xyy = vec3(u_resolution.x, u_resolution.y, u_resolution.y)
                            // The ray direction setup from shadertoy is often to map screen coords to a normalized view.
                            // Let's use a simpler ray direction for a start or adapt carefully.
                            // A common way for a ray direction through pixel (vUv):
                            vec2 uv_norm = vUv * 2.0 - 1.0; // -1 to 1
                            uv_norm.x *= R.x / R.y; // Aspect correction
                            vec3 rayDir = normalize(vec3(uv_norm, -1.0)); // Ray pointing into the screen

                            vec3 p = z_depth * rayDir;
                            
                            // Shift camera and get reflection coordinates
                            // Original: r = max(-++p, 0.).y; -> This modifies p, be careful.
                            // Let's try to keep p immutable for a moment in the ++p part for clarity.
                            vec3 p_temp_for_r = p;
                            p_temp_for_r.y += 1.0; // Simulating part of -++p for y component if p.y starts negative relative to some surface
                                                   // This part is very context dependent on the scene being raymarched.
                                                   // The original shader implies a scene where p.y can be negative.
                            
                            r_reflect = max(-p_temp_for_r.y, 0.0); // Simpler interpretation of reflection trigger
                                                              // The original was likely specific to its scene structure.

                            // Mirror
                            p.y += r_reflect + r_reflect;
                            
                            // Sine waves
                            for(d_step=1.; d_step<30.0; d_step+=d_step) // d_step was 'd'
                                p.y += cos(p*d_step + 2.*time_adjusted*cos(d_step + u_mid * 2.0)+z_depth).x/d_step;
                            
                            // Step forward (reflections are softer)
                            // Original: z += d = (.1*r+abs(p.y-1.)/(++r*r) + max(d=p.z+3.,-d*.1))/8.;
                            // This is complex distance estimator. Let's simplify or be careful.
                            // 'd' was d_step from inner loop.
                            // The 'd=p.z+3' is a distance function to a plane at z=-3
                            float dist_to_plane = p.z + 3.0 + u_treble * 2.0; // Audio influence
                            float scene_dist = abs(p.y-1.0) / ( (r_reflect + 1.0)*(r_reflect + 1.0) ); // Simplified from original
                            scene_dist = min(scene_dist, dist_to_plane);
                            d_step = (0.1 * r_reflect + scene_dist + max(dist_to_plane, -dist_to_plane*0.1) ) / 8.0;
                            d_step = max(0.01, d_step); // Ensure minimum step
                            z_depth += d_step;
                        }
                        // Tanh tonemapping
                        O_accum = tanh(O_accum/9e2);
                        gl_FragColor = vec4(O_accum.rgb, 1.0);
                    }
                `,
                initUniforms: () => ({
                    u_time: { value: 0.0 },
                    u_resolution: { value: new THREE.Vector2(window.innerWidth, window.innerHeight) },
                    u_bass: { value: 0.0 }, u_mid: { value: 0.0 }, u_treble: { value: 0.0 }, u_overall_avg: { value: 0.0 }
                })
            }
        };
        
        function initThreeJS() {
            console.log("initThreeJS: Initializing Scene, Camera, Renderer...");
            scene = new THREE.Scene();
            clock = new THREE.Clock();
            camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
            camera.position.set(0,0,0.1); // Important for DeviceOrientationControls relative movement

            renderer = new THREE.WebGLRenderer({ antialias: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            document.getElementById('container').appendChild(renderer.domElement);

            deviceControls = new DeviceOrientationControls(camera);
            console.log("initThreeJS: DeviceOrientationControls initialized.");

            // Create a single plane and material that we will update
            const planeGeometry = new THREE.PlaneGeometry(20, 20 * (window.innerHeight/window.innerWidth)); // Adjust size as needed
            currentShaderMaterial = new THREE.ShaderMaterial({
                vertexShader: commonVertexShader,
                // Fragment shader and uniforms will be set by setActiveVisual
                side: THREE.DoubleSide // Useful for some shaders
            });
            shaderPlane = new THREE.Mesh(planeGeometry, currentShaderMaterial);
            shaderPlane.position.set(0, 0, -5); // Position it in front of the camera
            scene.add(shaderPlane);
            shaderPlane.visible = false; // Initially hidden until a shader is chosen

            window.addEventListener('resize', onWindowResize, false);
            console.log("initThreeJS: Complete.");
        }

        function onWindowResize() {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
            if (currentShaderMaterial && currentShaderMaterial.uniforms.u_resolution) {
                currentShaderMaterial.uniforms.u_resolution.value.set(window.innerWidth, window.innerHeight);
            }
             // Adjust plane aspect ratio on resize
            if (shaderPlane) {
                const aspect = window.innerWidth / window.innerHeight;
                shaderPlane.geometry.dispose(); // Dispose old geometry
                shaderPlane.geometry = new THREE.PlaneGeometry(20, 20 / aspect); // Keep width 20, adjust height
            }
        }

        function animate() {
            requestAnimationFrame(animate);
            const delta = clock.getDelta();
            const elapsedTime = clock.getElapsedTime();

            if (deviceControls) deviceControls.update();
            const audioData = updateAudioSummary();

            if (currentShaderMaterial && currentShaderMaterial.uniforms.u_time) { // Check if uniforms exist
                currentShaderMaterial.uniforms.u_time.value = elapsedTime;
                currentShaderMaterial.uniforms.u_bass.value = audioData.bassAverage;
                currentShaderMaterial.uniforms.u_mid.value = audioData.midAverage;
                currentShaderMaterial.uniforms.u_treble.value = audioData.trebleAverage;
                currentShaderMaterial.uniforms.u_overall_avg.value = audioData.overallAverage;
            }
            
            if (videoTexture) videoTexture.needsUpdate = true; 
            renderer.render(scene, camera);
        }
        
        window.setActiveVisual = (type) => {
            console.log(`setActiveVisual: type="${type}", currentType="${currentVisualType}"`);
            if (type === 'none') {
                if (shaderPlane) shaderPlane.visible = false;
                currentVisualType = 'none';
                document.querySelectorAll('#controls button').forEach(btn => btn.classList.remove('active'));
                document.getElementById('btn-none')?.classList.add('active');
                console.log("Visuals cleared.");
                return;
            }

            const visualConfig = shaderVisuals[type];
            if (!visualConfig) {
                console.warn("Unknown visual type:", type);
                return;
            }

            if (currentVisualType === type && shaderPlane && shaderPlane.visible) {
                console.log("Visual already active:", type);
                return;
            }
            
            currentShaderMaterial.vertexShader = commonVertexShader; // Or visualConfig.vertexShader if different
            currentShaderMaterial.fragmentShader = visualConfig.fragmentShader;
            currentShaderMaterial.uniforms = THREE.UniformsUtils.clone(visualConfig.initUniforms());
            // Ensure resolution is current for new material
            currentShaderMaterial.uniforms.u_resolution.value.set(window.innerWidth, window.innerHeight);
            currentShaderMaterial.needsUpdate = true;
            
            if (shaderPlane) shaderPlane.visible = true;
            currentVisualType = type;

            document.querySelectorAll('#controls button').forEach(btn => btn.classList.remove('active'));
            document.getElementById(`btn-${type}`)?.classList.add('active');
            console.log("Switched to visual:", type);
        };
        
        async function requestPermissionsAndStart() { /* ... same permission logic ... */ 
            console.log("Requesting permissions and starting...");
            if(loadingMessage) loadingMessage.style.display = 'block';
            if(loadingMessage) loadingMessage.innerHTML = "Requesting Permissions...";

            let orientationPermissionGranted = false;
            if (typeof DeviceOrientationEvent !== 'undefined' && typeof DeviceOrientationEvent.requestPermission === 'function') {
                try {
                    const permissionState = await DeviceOrientationEvent.requestPermission();
                    if (permissionState === 'granted') {
                        orientationPermissionGranted = true; console.log("Device Orientation permission granted.");
                    } else {
                        console.warn("Device Orientation permission denied.");
                        if(loadingMessage) loadingMessage.innerHTML += "<br><small>Orientation permission denied.</small>";
                    }
                } catch (error) {
                    console.error("Error requesting Device Orientation permission:", error);
                     if(loadingMessage) loadingMessage.innerHTML += "<br><small>Error with Orientation permissions.</small>";
                }
            } else {
                orientationPermissionGranted = true; 
                console.log("Device Orientation permission not required or already available.");
            }

            if(loadingMessage) loadingMessage.innerHTML = "Initializing Audio & Video...";
            await startAudio(); 
            await initVideoStream(); 

            if(loadingMessage) loadingMessage.innerHTML = "Initializing 3D Scene...";
            initThreeJS(); 
            
            if (!orientationPermissionGranted && deviceControls) {
                console.warn("DeviceOrientationControls might not work without permission.");
            }

            if(loadingMessage) loadingMessage.style.display = 'none';
            if(controlsDiv) controlsDiv.style.display = 'block'; 
            
            setActiveVisual('shader1'); // Default to first shader
            animate(); 
        }

        if (startArButton) { /* ... same ... */ }
        });
    </script>
</body>
</html>

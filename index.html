<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no, viewport-fit=cover">
    <title>Gyro Shader AR Illusion</title>
    <style>
        body { margin: 0; font-family: sans-serif; overflow: hidden; background-color: #000; color: #fff; }
        #container { width: 100vw; height: 100vh; position: absolute; top: 0; left: 0; z-index: 1; }
        #videoBG {
            position: absolute; top: 0; left: 0;
            width: 100%; height: 100%;
            object-fit: cover;
            z-index: 0; /* Ensure it's behind the Three.js canvas if canvas is transparent */
        }
        #controls { /* Hide controls as we have only one visual for now */
            display: none; 
            position: absolute; top: 10px; left: 10px; z-index: 100;
            background: rgba(0,0,0,0.6); padding: 8px; border-radius: 8px;
        }
        #loadingMessage, #startExperienceUI {
            position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%);
            text-align: center; background: rgba(0,0,0,0.85);
            padding: 30px; border-radius: 15px; z-index: 200;
        }
        #loadingMessage { display: none; font-size: 1.5em; }
        #startExperienceUI h2 { margin-top: 0; color: #00aaff; }
        #startExperienceUI p { font-size: 1.1em; margin-bottom: 25px; line-height: 1.5; }
        #startArButton {
            padding: 15px 30px; font-size: 1.25em; cursor: pointer;
            background-color: #007bff; color: white; border: none;
            border-radius: 8px; font-weight: bold;
        }
    </style>

    <script type="importmap">
    {
      "imports": {
        "three": "https://cdnjs.cloudflare.com/ajax/libs/three.js/0.151.0/three.module.min.js",
        "three/addons/": "https://unpkg.com/three@0.151.0/examples/jsm/"
      }
    }
    </script>
</head>
<body>
    <div id="startExperienceUI">
        <h2>Gyro Shader Experience</h2>
        <p>
            This experience uses your phone's camera for the background,
            motion sensors to look around, and an audio-reactive shader.
            <br><br>
            Please click "Start" and allow all requested permissions.
        </p>
        <button id="startArButton">Start Experience</button>
    </div>

    <div id="loadingMessage">Loading...</div>
    <video id="videoBG" autoplay playsinline muted style="display:none;"></video>
    <div id="controls">
        </div>
    <div id="container"></div>

    <script type="module">
        import * as THREE from 'three';
        import { DeviceOrientationControls } from 'three/addons/controls/DeviceOrientationControls.js';

        let scene, camera, renderer, clock, deviceControls;
        let videoTexture, videoElement;
        let shaderMaterial; // To hold our custom shader material

        const loadingMessage = document.getElementById('loadingMessage');
        const startExperienceUI = document.getElementById('startExperienceUI');
        const startArButton = document.getElementById('startArButton');
        videoElement = document.getElementById('videoBG');

        // --- Audio Analysis ---
        let analyser;
        let dataArray;
        const audioSummary = { overallAverage: 0, bassAverage: 0, midAverage: 0, trebleAverage: 0, dataArrayValid: false, audioContextActive: false };

        async function startAudio() {
            if (audioSummary.audioContextActive) return;
            console.log("Requesting microphone access...");
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const source = audioContext.createMediaStreamSource(stream);
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256; 
                analyser.smoothingTimeConstant = 0.7;
                source.connect(analyser);
                const bufferLength = analyser.frequencyBinCount;
                dataArray = new Uint8Array(bufferLength);
                audioSummary.dataArrayValid = true;
                audioSummary.audioContextActive = true;
                console.log("Microphone access granted and analyser active.");
            } catch (err) {
                console.error("Error accessing microphone:", err);
                if(loadingMessage) loadingMessage.innerHTML += "<br><small>Microphone access denied. Audio reactivity disabled.</small>";
            }
        }

        function updateAudioSummary() {
            if (!analyser || !dataArray || !audioSummary.dataArrayValid) {
                audioSummary.overallAverage = 0; audioSummary.bassAverage = 0; audioSummary.midAverage = 0; audioSummary.trebleAverage = 0;
                return audioSummary;
            }
            analyser.getByteFrequencyData(dataArray);
            let S=0,B=0,M=0,T=0,l=dataArray.length,bc=Math.floor(l*0.15),mc=Math.floor(l*0.5);
            for(let i=0;i<l;i++){const v=dataArray[i]; S+=v; if(i<bc)B+=v;else if(i<mc)M+=v;else T+=v;}
            audioSummary.overallAverage=(S/l||0)/255; audioSummary.bassAverage=(bc>0?B/bc:0)/255; // Normalize to 0-1
            audioSummary.midAverage=((mc-bc)>0?M/(mc-bc):0)/255; audioSummary.trebleAverage=((l-mc)>0?T/(l-mc):0)/255;
            return audioSummary;
        }
        
        async function initVideoStream() {
            console.log("initVideoStream: Attempting to get camera feed.");
            if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
                try {
                    const constraints = { video: { facingMode: 'environment' } };
                    const stream = await navigator.mediaDevices.getUserMedia(constraints);
                    videoElement.srcObject = stream;
                    await videoElement.play();
                    videoElement.style.display = 'block';
                    console.log("initVideoStream: Camera stream acquired and playing.");
                    
                    videoTexture = new THREE.VideoTexture(videoElement);
                    videoTexture.minFilter = THREE.LinearFilter;
                    videoTexture.magFilter = THREE.LinearFilter;
                    // No format needed for background texture usually.
                    if (scene) scene.background = videoTexture;

                } catch (err) {
                    console.error("Error accessing camera:", err);
                    if(loadingMessage) loadingMessage.innerHTML = "Could not access camera. Check permissions.<br><small>" + err.message + "</small>";
                    if (scene) scene.background = new THREE.Color(0x100015); // Dark fallback
                }
            } else {
                console.error("getUserMedia not supported!");
                if(loadingMessage) loadingMessage.innerHTML = "Camera access not supported on this browser.";
                if (scene) scene.background = new THREE.Color(0x100015); // Dark fallback
            }
        }

        function initThreeJS() {
            console.log("initThreeJS: Initializing Scene, Camera, Renderer...");
            scene = new THREE.Scene();
            clock = new THREE.Clock();

            const aspectRatio = window.innerWidth / window.innerHeight;
            camera = new THREE.PerspectiveCamera(75, aspectRatio, 0.1, 1000);
            camera.position.set(0, 0, 0.1); // Start close to origin, controls will orient

            renderer = new THREE.WebGLRenderer({ antialias: true }); // alpha:true might not be needed if video bg works
            renderer.setSize(window.innerWidth, window.innerHeight);
            document.getElementById('container').appendChild(renderer.domElement);

            deviceControls = new DeviceOrientationControls(camera);
            console.log("initThreeJS: DeviceOrientationControls initialized.");

            createShaderVisual(); // Create and add the shader plane

            window.addEventListener('resize', onWindowResize, false);
            console.log("initThreeJS: Complete.");
        }

        function onWindowResize() {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
            if (shaderMaterial) {
                shaderMaterial.uniforms.u_resolution.value.set(window.innerWidth, window.innerHeight);
            }
        }

        function animate() {
            requestAnimationFrame(animate);
            const delta = clock.getDelta();
            const elapsedTime = clock.getElapsedTime();

            if (deviceControls) deviceControls.update();
            const audioData = updateAudioSummary();

            if (shaderMaterial) {
                shaderMaterial.uniforms.u_time.value = elapsedTime;
                shaderMaterial.uniforms.u_bass.value = audioData.bassAverage;
                shaderMaterial.uniforms.u_mid.value = audioData.midAverage;
                shaderMaterial.uniforms.u_treble.value = audioData.trebleAverage;
                shaderMaterial.uniforms.u_overall_avg.value = audioData.overallAverage;
            }
            
            if (videoTexture) videoTexture.needsUpdate = true; 
            renderer.render(scene, camera);
        }

        const vertexShader = `
            varying vec2 vUv;
            void main() {
                vUv = uv;
                gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
            }
        `;

        const fragmentShader = `
            precision highp float;
            uniform vec2 u_resolution;
            uniform float u_time;
            uniform float u_bass;       // Audio uniform (0-1 range)
            uniform float u_mid;        // Audio uniform (0-1 range)
            uniform float u_treble;     // Audio uniform (0-1 range)
            uniform float u_overall_avg; // Audio uniform (0-1 range)
            varying vec2 vUv;

            // User's shader code, adapted:
            // Original mainImage( out vec4 o, vec2 u )
            void main() {
                vec2 fragCoord = vUv * u_resolution; // Equivalent to 'u' in Shadertoy if plane covers viewport correctly
                                                    // OR, more directly if plane UVs map to screen UVs
                vec2 u = vUv; // Use vUv directly if it's already screen-like (0-1)
                              // The shader manipulates 'u' heavily, so starting with vUv is standard.
                
                // The shader's internal normalization of 'u':
                u = .2*( (vUv * 2.0 - 1.0) * u_resolution )/u_resolution.y; 
                // This makes 'u' aspect-corrected and normalized where y goes from ~-0.2 to 0.2 for a square if width > height

                vec4 z_shadertoy, o_shadertoy; // Shadertoy 'z' and 'o'
                o_shadertoy = vec4(1.0,2.0,3.0,0.0); // Initial 'o' from shader
                z_shadertoy = o_shadertoy;           // Initial 'z' from shader

                vec2 v_shadertoy = u_resolution.xy; // Corresponds to 'v' in the shader

                // --- Start of user's shader logic ---
                // 't' is time, let's make it audio reactive for a demo
                float t = u_time + u_bass * 5.0; // MODIFIED: u_time is iTime, u_bass influences it

                for (float a = .5, i = 0.0; // loop variable 'i' should start at 0 for typical for loops
                    ++i < 19.; // Loop 18 times (i from 1 to 18)
                    o_shadertoy += (1. + cos(z_shadertoy+t)) 
                        / length((1.+i*dot(v_shadertoy,v_shadertoy)) 
                                * sin(1.5*u/(.5-dot(u,u)) - 9.*u.yx + t))
                    )  
                    v_shadertoy = cos(++t - 7.*u*pow(a += .03, i)) - 5.*u, 
                    u += tanh(40. * dot(u *= mat2(cos(i + .02*t - vec4(0,11,33,0))) 
                                            ,u) 
                                * cos(1e2*u.yx + t)) / 2e2
                    + .2 * a * u
                    + cos(4./exp(dot(o_shadertoy,o_shadertoy)/1e2) + t) / 3e2;
                    
                o_shadertoy = 25.6 / (min(o_shadertoy, 13.) + 164. / o_shadertoy) 
                    - dot(u, u) / 250.;
                // --- End of user's shader logic ---

                gl_FragColor = vec4(o_shadertoy.rgb, 1.0); // Ensure alpha is 1 for opaque plane
            }
        `;

        function createShaderVisual() {
            console.log("Creating shader visual...");
            // A large plane to act as the "canvas" for the shader
            // Position it in front of the camera
            const planeGeometry = new THREE.PlaneGeometry(20, 20 * (window.innerHeight/window.innerWidth), 1, 1); 
            
            shaderMaterial = new THREE.ShaderMaterial({
                vertexShader: vertexShader,
                fragmentShader: fragmentShader,
                uniforms: {
                    u_time: { value: 0.0 },
                    u_resolution: { value: new THREE.Vector2(window.innerWidth, window.innerHeight) },
                    u_bass: { value: 0.0 },
                    u_mid: { value: 0.0 },
                    u_treble: { value: 0.0 },
                    u_overall_avg: { value: 0.0 }
                },
                // side: THREE.DoubleSide // If you want to see it from behind too
            });

            const shaderPlane = new THREE.Mesh(planeGeometry, shaderMaterial);
            // Position the plane. If camera is at (0,0,0.1) looking towards -Z,
            // place plane at Z = -5 or so.
            shaderPlane.position.set(0, 0, -5); 
            
            scene.add(shaderPlane);
            console.log("Shader plane added to scene at z = -5");
        }


        async function requestPermissionsAndStart() {
            // ... (same permission logic as your previous 'gyro' version)
            console.log("Requesting permissions and starting...");
            if(loadingMessage) loadingMessage.style.display = 'block';
            if(loadingMessage) loadingMessage.innerHTML = "Requesting Permissions...";

            let orientationPermissionGranted = false;
            if (typeof DeviceOrientationEvent !== 'undefined' && typeof DeviceOrientationEvent.requestPermission === 'function') {
                try {
                    const permissionState = await DeviceOrientationEvent.requestPermission();
                    if (permissionState === 'granted') {
                        orientationPermissionGranted = true; console.log("Device Orientation permission granted.");
                    } else {
                        console.warn("Device Orientation permission denied.");
                        if(loadingMessage) loadingMessage.innerHTML += "<br><small>Orientation permission denied.</small>";
                    }
                } catch (error) {
                    console.error("Error requesting Device Orientation permission:", error);
                     if(loadingMessage) loadingMessage.innerHTML += "<br><small>Error with Orientation permissions.</small>";
                }
            } else {
                orientationPermissionGranted = true; 
                console.log("Device Orientation permission not required or already available.");
            }

            if(loadingMessage) loadingMessage.innerHTML = "Initializing Audio & Video...";
            await startAudio(); 
            await initVideoStream(); 

            if(loadingMessage) loadingMessage.innerHTML = "Initializing 3D Scene...";
            initThreeJS(); 
            
            if (!orientationPermissionGranted && deviceControls) {
                console.warn("DeviceOrientationControls might not work without permission.");
            }

            if(loadingMessage) loadingMessage.style.display = 'none';
            // Controls div is hidden by CSS for this example
            // if(controlsDiv) controlsDiv.style.display = 'block'; 
            
            animate(); 
        }

        if (startArButton) {
            startArButton.addEventListener('click', () => {
                if (startExperienceUI) startExperienceUI.style.display = 'none';
                if (loadingMessage) loadingMessage.style.display = 'block';
                requestPermissionsAndStart().catch(err => {
                    console.error("Error in requestPermissionsAndStart chain:", err);
                    if(loadingMessage) loadingMessage.innerHTML = "Failed to start experience: " + err.message;
                });
            });
        } else { console.error("Start Button not found!"); }

        });
    </script>
</body>
</html>
